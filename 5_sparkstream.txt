

Theory: Spark streaming uses Spark Core's fast scheduling capability to perform streaming analytics. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data. This design enables the same set of application code written for batch analytics to be used in streaming analytics, thus facilitating easy implementation of lambda architecture.

Implementation:
(Make a prac5 folder and do below processes)
# sbt package creation 
1.	Create a new folder logged in as Hadoop user
2.	Create file ‘NetworkWordCount.scala’ in that folder.  (nano NetworkWordCount.scala)
import org.apache.spark.SparkConf
import org.apache.spark.streaming._
object NetworkWordCount {
  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setMaster("local[2]")setAppName("NetworkWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(10))
    val lines = ssc.socketTextStream("localhost",9999)
    val words = lines.flatMap(_.split(" "))
    val tuples = words.map(word => (word ,1))
    val wordCounts = tuples.reduceByKey((t, v) => t + v)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }
}
3.	Create a ‘networkwordcount.sbt’ file in the same folder (nano networkwordcount.sbt)
(Check below versions by (spark-shell) command in terminal)
name := "networkwordcount"
version := "1.0.0"
scalaVersion := "2.12.15"
libraryDependencies += "org.apache.spark" % "spark-streaming_2.12" % "3.3.1" % "provided"

4.	Create the sbt package
$ sbt package

5.	Start the spark server. (To get ur worker id type(localhost:8080))
$ start-master.sh
$ start-worker.sh spark://Ubuntu.virtualbox.org:7077
6.	On a separate terminal, start the netscape server

$ nc -lk 9999
7.	On the original terminal, submit the scala program to spark
$ spark-submit --class ‘NetworkWordCount’ --master “spark://Ubuntu.myguest.virtualbox.org:7077” ./target/scala- 2.12/networkwordcount_2.12-1.0.0.jar
8.	On the netscape terminal provide textual input for the scala progra
